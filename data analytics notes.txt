data analytics
    Analytics programs allow businesses to access the untapped value locked within their data.
    We collect data from many sources to derive insights
    data anlytics is growing because of growing data, cheaper storage and growing computing power
    data can be structural data or unstructured data, 
    stractural data is rectangular and can be represented in a table, eg. saving a persons details, name , surname, id 
    unstructured data cannot be represented as a table, such as images, audio and video and cannot be added to a spread sheet 

data types.

    A data element is an attribute about a person, place, or thing containing data within a range of values. 
    Data elements also describe characteristics of activities, including orders, transactions, and events. 
    A data type limits the values a data element can have
    Individual data types support structured, unstructured, and semi-structured data
    Tabular data is data organized into a table, made up of columns and rows
    Structured data is tabular in nature and organized into rows and columns.
    The character data type limits data entry to only valid characters. 
    Characters can include the alphabet that you might see on your keyboard, as well as numbers.
    Alphanumeric is the most widely used data type for storing character-based data.
    Strong typing is when technology rigidly enforces data types
    Weak typing loosely enforces data types

qualitative & quantitative data

    Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative
    Qualitative data consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative

discrete and continuous

    discrete data represents measurements that can't be subdivided.
    when you measure things like height and weight, you are collecting continuous data

Relational and nonrelation data

    A nonrelational database does not have a predefined structure based on tabular data
    Relational databases excel at storing and processing structured data
    The need to interact with unstructured data is one of the reasons behind the rise of nonrelational databases
    Cardinality refers to the relationship between two entities, showing how many instances of one entity relate to instances in another entity
    A unary relationship is when an entity has a connection with itself. For example, a unary relationship where a single manager has multiple employees.

    Relational
        consistancy
        security
        ease of backup
    
    nonrelational data is stored on ke-value basis
        flexibility
        scalibility
        cost effective  

key value database

    A key-value database is one of the simplest ways of storing data. 
    Data is stored as a collection of keys and their corresponding values. 
    A key must be globally unique across the entire database.
    The use of keys differs from a relational database, where a given key identifies an individual row in a specific table. 
    There are no structural limits on the values of a key. 
    A key can be a sequence of numbers, alphanumeric strings, or some other combination of values. 
    The data that corresponds with a key can be any structured or unstructured data type

document database

    A document database is similar to a key-value database, with additional restrictions
    In a key-value database, the value can contain anything. 
    With a document database, the value is restricted to a specific structured format

column-family database

    Column-family databases use an index to identify data in groups of related columns.

Graph databases

    Graph databases specialize in exploring relationships between pieces of data.
    Relational models focus on mapping the relationships between entities. 
    Graph models map relationships between actual pieces of data
    Understanding the connection between products is a challenge that graphs solve with ease.

Database use cases

    Different business needs require different database designs. 
    While all databases store data, the database's structure needs to match its intended purpose
    Business requirements impact the design of individual tables and how they are interconnected

    Databases tend to support two major categories of data processing: 
        Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP).

        Online Transactional Processing
        OLTP systems handle the transactions we encounter every day
        OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design
        Typically, OLTP databases are in 3NF.
    Normalization-Normalization is a process for structuring a database in a way that minimizes duplication of data.
    One of the principles is that a given piece of data is stored once and only once.
    First normal form (1NF) is when every row in a table is unique and every column contains a unique value
    Second normal form (2NF) starts where 1NF leaves off. In addition to each row being unique, 
    2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key
    Third normal form (3NF) builds upon 2NF by adding a rule stating all columns must depend on only the primary key.

    Online analytical Processing
        OLAP systems focus on the ability of organizations to analyze data.
        databases that power OLAP systems have a denormalized design
        denormalization results in wider tables than those found in an OLTP database
    
    Schema Concepts
    The design of a database schema depends on the purpose it serves.
    Data warehouses serve the entire organization
    A data mart is a subset of a data warehouse  
    data marts focus on the needs of a particular department within the organization.
    A data lake stores raw data in its native format instead of conforming to a relational database structure
    Using a data lake requires additional knowledge about the raw data to make it analytically useful

Data acquisition Concepts
    Data can come from internal systems you operate, or you can obtain it from third-party sources

    Integration
    You need to retrieve, reshape, and insert data to move data between operational and analytical environments

        ETL
            Extract:  In the first phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.
            Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.
            Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.

        Data Collection Methods

        application programming interface (API)
        A structured method for computer systems to exchange information. 
        APIs provide a consistent interface to calling applications, regardless of the internal database structure.
        APIs represent a specific piece of business functionality.

    Working with data
    Whenever a SQL query executes, the database has to parse the query. 
    Parsing translates the human-readable SQL into code the database understands


Data Quality Challenges

    Duplicate Data
    Duplicate data occurs when data representing the same transaction is accidentally duplicated within a system.
    Humans are primarily responsible for creating duplicate data

    Redundant Data
    redundant data happens when the same data elements exist in multiple places within a system
    Frequently, data redundancy is a function of integrating multiple systems.
    There are several options for resolving redundant data. 
    One approach synchronizes changes to shared data elements between the Accounting and Sales systems

    Missing Values
    Another issue that impacts data quality is the concept of missing values. 
    Missing values occur when you expect an attribute to contain data but nothing is there. 
    A null value is the absence of a value. A null is not a space, blank, or other character
    
    Invalid Data
    Invalid data are values outside the valid range for a given attribute. 
    An invalid value violates a business rule instead of having an incorrect data type
    Invalid values violate business rules, not technical rules. 
    For example, â€“99,999 is a valid number, but it is an invalid temperature for a location on Earth

    Nonparametric Data
    Nonparametric data is data collected from categorical variables

    Data Outliers
    A data outlier is a value that differs significantly from other observations in a dataset
    With outliers, you need to understand why they exist and whether they are valid in the context of your analysis.

    Specification Mismatch
    A specification describes the target value for a component. 
    A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values.

    Data Type Validation
    Data type validation ensures that values in a dataset have a consistent data type.
    
Data Manipulation Techniques

    Recoding data
    Recoding data is a technique you can use to map original values for a variable into new values to facilitate analysis
    Recoding groups data into multiple categories, creating a categorical variable
    A categorical variable is either nominal or ordinal.
    Nominal variables are any variable with two or more categories where there is no natural order of the categories, like hair color or eye color
    Ordinal variables are categories with an inherent rank. For example, T-shirt size is an example of an ordinal variable, as sizes come in small, medium, large, and extra-large

    Derived variables
    A derived variable is a new variable resulting from a calculation on an existing variable

    Data Merge
    A data merge uses a common variable to combine multiple datasets with different structures into a single datase
    Merging data improves data quality by adding new variables to your existing data
    Since a data merge adds columns to a dataset, merging gives you additional data about a specific observation

    Data Blending
    a blending combines multiple sources of data into a single dataset at the reporting layer.
    blending is conceptually similar to the extract, transform, and load process
    Data blending differs from ETL in that it allows an analyst to combine datasets in an ad hoc manner without saving the blended dataset in a relational database. 
    Instead of the blended dataset persisting over time, it exists only at the reporting layer, not in the source databases.

    Concatenation
    Concatenation is the merging of separate variables into a single variable
    Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns
    The need for concatenation frequently occurs when dealing with date and time data
    Concatenation is also useful when generating address information

    Data append
    A data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets. 
    When appending data, you save the result as a new dataset for ongoing analysis.

    Imputation
        Imputation is a technique for dealing with missing values by replacing them with substitutes.

        Remove Missing Data:  With this approach, you can remove rows with missing values without impacting the quality of your overall analysis.

        Replace with Zero:  With this approach, you replace missing values with a zero. Whether or not it is appropriate to replace missing data with a zero is contextual. 
        In this case, zero isn't an appropriate value, as a person's weight should be a positive number. 
        In addition, replacing a zero in this case has an extraordinary impact on the overall average weight.

        Replace with Overall Average:  Instead of using a zero, you can compute the average Weight value for all rows that have data 
        and then replace the missing Weight values with that calculated average.
        Replace with Most Frequent (Mode):  Alternatively, you can take the most frequently occurring value, called the mode, and use that as the constant.

        Closest Value Average:  With this approach, you use the values from the rows before and after the missing values. 

    Reduction
    When dealing with big data, it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis. 
    Reduction is the process of shrinking an extensive dataset without negatively impacting its analytical value

    Dimensionality Reduction
    One reduction technique is dimensionality reduction, which removes attributes from a dataset
    Removing attributes reduces the dataset's overall size.

    Numerosity Reduction
    Another technique is numerosity reduction, which reduces the overall volume of data.
    Sampling is a technique that selects a subset of individual records from the initial dataset

    Agregation
    Data aggregation is the summarization of raw data for analysis. 
    When you are dealing with billions of individual records, a data summary can help you make sense of it all
    Aggregating data provides answers that help make decisions.

    Transposition
    Transposing data is when you want to turn rows into columns or columns into rows to facilitate analysis

    Normalizing
    In the context of data manipulation, normalizing data differs from our discussion of database normalization
    normalizing data converts data from different scales to the same scale. 

Managing data quality

    Circumstances to Check for Quality
    There are numerous circumstances where it is appropriate to implement data quality control checks.
    Every stop along the data life-cycle journey can impact data quality. 
    Errors during data acquisition, transformation, manipulation, and visualization all contribute to degrading data quality.

    Automated Validation
    Whether source data is machine- or human-generated, 
    one way to prevent data entry mistakes from adversely impacting data quality is to automate data validation checks.

    Data Quality Dimensions
    Six dimensions to take into account when assessing data quality are accuracy, completeness, consistency, timeliness, uniqueness, and validity.

    Data Quality Rules and Metrics
    With an understanding of data quality dimensions, you need to consider how to measure each of them in your quest to improve overall quality

    Methods to Validate Quality

    reasonable expectations - 
    One approach is to determine whether or not the data in your analytics environment meets your reasonable expectations

    Data Profiling -
    Another approach to improving quality is to profile your data. 
    Data profiling uses statistical measures to check for data discrepancies, including values that are missing, that occur either infrequently or too frequently, or that should be eliminated. 
    Profiling can also identify irregular patterns within your data. 

    data audits
    Another method to keep in mind is auditing your data. 
    Data audits look at your data and help you understand whether or not you have the data you need to operate your business. 
    Data audits use data profiling techniques and can help identify data integrity and security issues

    Sampling
    Another method for validating data quality is by examining a sample of your data. 
    Sampling is a statistical technique in which you use a subset of your data to inform conclusions about your overall data

    Cross-validation
    Cross-validation is a statistical technique that evaluates how well predictive models perform. 
    Cross-validation works by dividing data into two subsets. 
    The first subset is the training set, and the second is the testing, or validation, set.

statistics

    Descriptive statistics is a branch of statistics that summarizes and describes data.

    Measures of Frequency
    Measures of frequency help you understand how often something happens

    count- the total number of observations 
    percentage- is a frequency measure that identifies the proportion of a given value for
    a variable with respect to the total number of rows in the dataset
    frequency- Frequency describes how often a specific value for a variable occurs in a dataset.

    Measures of Central Tendency
    To help establish an overall perspective on a given dataset, an analyst explores various measures of central tendency. 
    You use measures of central tendency to identify the central, or most typical, value in a dataset

    The mean, or average, is a measurement of central tendency that computes the arithmetic average for a given set of numeric values
    calculating mean - You sum all sample observations for a variable and then divide by the number of observations

    The median Another measurement of central tendency is the median, which identifies the midpoint value for all observations of a variable
    The first step to calculating the median is sorting your data numerically. 
    Once you have an ordered list of values, the next step depends on whether you have an even or an odd number of observations for a variable.

    The mode is a variable's most frequently occurring observation. Depending on your data, you may not have a mode.

    Measures of Dispersion
    You use measures of dispersion to create context around data spread.

    The range of a variable is the difference between its maximum and minimum values
    In statistics, a probability distribution, or distribution, is a function that illustrates probable values for a variable, and the frequency with which they occur.
    The normal distribution is symmetrically dispersed around its mean, which gives it a distinctive bell-like shape

    A skewed distribution has an asymmetrical shape, with a single peak and a long tail on one side. 
    Skewed distributions have either a right (positive) or left (negative) skew. 
    When the skew is to the right, the mean is typically greater than the median. 
    On the other hand, a distribution with a left skew typically has a mean less than the median.

    A bimodal distribution has two distinct modes, whereas a multimodal distribution has multiple distinct modes. When you visualize a bimodal distribution, you see two separate peaks

    Variance is a measure of dispersion that takes the values for each observation in a dataset and calculates how far away they are from the mean value
    This dispersion measure indicates how spread out the data is in squared units

    Standard deviation is a statistic that measures dispersion in terms of how far values of a variable are from its mean.
    standard deviation is the average deviation between individual values and the mean

    Each Sample is Unique-  each sample from a population is unique
    Suppose you take two different samples from a population. The variance and standard deviation for each sample will be different.
    The Central Limit Theorem and empirical rule combine to make the normal distribution the most important distribution in statistics.
    There are two special normal distributions 

    The standard normal distribution, or Z-distribution, is a special normal distribution with a mean of 0 and a standard deviation of 1

    The Student's t-distribution, commonly known as the t-distribution, is similar to the standard normal distribution in that it has a mean of 0 with a bell-like shape.

    Measures of Position

    Inferential statistics
    Inferential statistics is a branch of statistics that uses sample data to draw conclusions about the overall population

    Confidence Intervals
    Each time you take a sample from a population, the statistics you generate are unique to the sample
    A confidence interval describes the possibility that a sample statistic contains the true population parameter in a range of values around the mean.
    When calculating a confidence interval, you end up with a lower bound value(lower limit) and an upper bound value(upper limit).

    Hypothesis Testing
    A hypothesis test consists of two statements, only one of which can be true. 
    It uses statistical analysis of observable data to determine which of the two statements is most likely to be true
    
    
    Simple Linear Regression
    Simple linear regression is an analysis technique that explores the relationship between an independent variable and a dependent variable

    From Simple to Multiple Linear Regression

    analysis techniques
    

Statistics Packages

    IBM SPSS- One of the most popular pieces of statistical software is IBM's SPSS package.
    IBM's SPSS Modeler is one popular tool for building graphical machine learning models. 
    Instead of requiring that users write code, it provides an intuitive interface where analysts 
    can select the tasks that they would like the software to carry out and then connect them in a flowchart-style interface.

    Stata- another statistical analysis package
    the same features as SPSS and SAS and provides users with both a graphical interface and a command-line interface depending on their personal preference
    Minitab- Minitab shares most of the same features as SPSS, SAS, and Stata but fits into the same category as Stata - an older tool that is not widely used today.

    RapidMiner
    RapidMiner is another graphical machine learning tool that works in a manner similar to IBM SPSS Modeler. 
    It offers access to hundreds of different algorithms that may be placed in a visually designed machine-learning workflow. 
    RapidMiner also offers prebuilt analytic templates for common business scenarios

    Analytics Suites

    IBM Cognos It uses a web-based platform to offer analysts within an organization access to their data and is backed by IBM's Watson artificial intelligence capability

    Microsoft Power BI Power BI is Microsoft's analytics suite built on the company's popular SQL Server database platform.

    MicroStrategy offers many of the same tools as its counterparts, making it easy for users to build dashboards and reports and apply machine learning techniques to their business data.

    Domo is a software-as-a-service (SaaS) analytics platform that allows businesses to ingest their data and apply a variety of analytic and modeling capabilities.

    Salesforce Datorama is an analytics tool that focuses on a specific component of an organization's business: sales and marketing.

    AWS QuickSight is a dashboarding tool available as part of the Amazon Web Services cloud offering.

    Tableau- The focus of this tool is on the easy ingestion of data from a wide variety of sources and powerful visualization capabilities that allow analysts 
    and business leaders to quickly identify trends in their data and drill down into specific details.

    Qlik is another popular SaaS analytics platform, offering access to cloud-based analytics capabilities.

    BusinessObjects is an enterprise reporting tool from SAP that is designed to provide a comprehensive reporting and analytics environment for organizations.

Data visualisation- Reports and dashboards both summarize data for end users

Understanding the Business Requirements
A report is a static electronic or physical document that reflects information at a given point in time
a dashboard is an interactive visualization that encourages people to explore data dynamically.

With a pull approach, you publish a report to a known location, like a web page, and let people know the frequency and timing of when the report updates.
With a push approach, the report is automatically sent to the appropriate people as it becomes available
With the blended approach, you inform people that the report is available while maintaining central control of the report itself.





